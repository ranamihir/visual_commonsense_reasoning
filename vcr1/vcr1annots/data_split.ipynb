{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import json\n",
    "import subprocess\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = 0.85\n",
    "answer_types = ['answer', 'rationale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'sample' | 'mini'\n",
    "size_type = 'orig'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_val(train_size, size_type):\n",
    "    with open('train-orig.jsonl', 'r') as f_in:\n",
    "        data = np.array([json.loads(s) for s in f_in])\n",
    "    \n",
    "    data_grp_movies = OrderedDict()\n",
    "    for i in range(len(data)):\n",
    "        data_grp_movies.setdefault(data[i]['movie'], []).append(i)\n",
    "    \n",
    "    movies = list(data_grp_movies.keys())\n",
    "    \n",
    "    if 0. <= train_size <= 1.:\n",
    "        train_size = int(train_size*len(movies))\n",
    "    \n",
    "    train_movie_ind = np.random.choice(range(len(movies)), size=train_size, replace=False)\n",
    "    val_movie_ind = np.setdiff1d(range(len(movies)), train_movie_ind)\n",
    "    \n",
    "    print(len(train_movie_ind), len(val_movie_ind))\n",
    "    \n",
    "    train, train_ind = [], []\n",
    "    for i in range(len(movies)):\n",
    "        if i in train_movie_ind:\n",
    "            train_ind.extend(data_grp_movies[movies[i]])\n",
    "            train.extend(data[data_grp_movies[movies[i]]])\n",
    "\n",
    "    val, val_ind = [], []\n",
    "    for i in range(len(movies)):\n",
    "        if i in val_movie_ind:\n",
    "            val_ind.extend(data_grp_movies[movies[i]])\n",
    "            val.extend(data[data_grp_movies[movies[i]]])\n",
    "            \n",
    "    indices = {'train': train_ind, 'val': val_ind}\n",
    "    split_data_dict = {'train': train, 'val': val}\n",
    "    \n",
    "    assert len(train) == len(train_ind)\n",
    "    assert len(val) == len(val_ind)\n",
    "    \n",
    "    print(len(train_ind), len(val_ind))\n",
    "    \n",
    "    for data_type in split_data_dict.keys():\n",
    "        with open('{}-{}.jsonl'.format(data_type, size_type), 'w') as f_out:\n",
    "            for line in split_data_dict[data_type]:\n",
    "                f_out.write(json.dumps(line)+'\\n')\n",
    "    \n",
    "    return train, val, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1653 292\n",
      "183413 29510\n"
     ]
    }
   ],
   "source": [
    "train, val, indices = split_train_val(train_size, size_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding_split(answer_type, size_type, indices):\n",
    "    group_items = {'train': [], 'val': []}\n",
    "    data_folder = '../../data/'\n",
    "    \n",
    "    with h5py.File('{}bert_da_{}_train_orig.h5'.format(data_folder, answer_type), 'r') as f:\n",
    "        for data_type in ['train', 'val']:\n",
    "            print(data_type)\n",
    "            for ind in indices[data_type]:\n",
    "                group_items[data_type].append({k: np.array(v, dtype=np.float16) \\\n",
    "                                               for k, v in f[str(ind)].items()})\n",
    "\n",
    "    for data_type in ['train', 'val']:\n",
    "        print(data_type)\n",
    "        with h5py.File('{}bert_da_{}_{}_{}.h5'.format(data_folder, answer_type, data_type, size_type), 'w') as f:\n",
    "            for ind in range(len(group_items[data_type])):\n",
    "                group = f.create_group(str(ind))\n",
    "                for k, v in group_items[data_type][ind].items():\n",
    "                    group.create_dataset(k, data=v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "answer\n",
      "train\n",
      "val\n",
      "train\n",
      "val\n",
      "rationale\n",
      "train\n",
      "val\n",
      "train\n",
      "val\n"
     ]
    }
   ],
   "source": [
    "for answer_type in answer_types:\n",
    "    print(answer_type)\n",
    "    create_embedding_split(answer_type, size_type, indices)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '../../data/'\n",
    "answer_type = 'answer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_items1 = {'train': [], 'val': []}\n",
    "for data_type in ['train', 'val']:\n",
    "    with h5py.File('{}bert_da_{}_{}_{}2.h5'.format(data_folder, answer_type, data_type, size_type), 'r') as f:\n",
    "        group_items1[data_type].append({k: np.array(v, dtype=np.float16) \\\n",
    "                                               for k, v in f[str(0)].items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_items2 = {'train': [], 'val': []}\n",
    "with h5py.File('{}bert_da_{}_train_orig.h5'.format(data_folder, answer_type), 'r') as f:\n",
    "    for data_type in ['train', 'val']:\n",
    "        group_items2[data_type].append({k: np.array(v, dtype=np.float16) \\\n",
    "                                       for k, v in f[str(indices[data_type][0])].items()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_type in ['train', 'val']:\n",
    "    for k, v in group_items1[data_type][0].items():\n",
    "        assert np.all(v == group_items2[data_type][0][k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
